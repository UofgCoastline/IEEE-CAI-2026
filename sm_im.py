#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆPyTorch DQN+MCTS+CNN-LSTMæµ·å²¸çº¿æ£€æµ‹ç³»ç»Ÿ
æ ¸å¿ƒç®—æ³•: NDWI+Otsu â†’ HSVæ©è†œ â†’ Cannyè¾¹ç¼˜ â†’ DQN+MCTSæ™ºèƒ½ä¼˜åŒ– â†’ CNN-LSTMè¿ç»­æ€§ä¿®å¤
æ”¯æŒ: Monte Carlo Tree Search, CNN-LSTM Pattern Memory, Advanced Connection Algorithm
"""

import os
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from scipy import ndimage
from scipy.ndimage import label, gaussian_filter
import random
from collections import deque, namedtuple
import math
from io import BytesIO

# PyTorch imports
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# å¯é€‰ä¾èµ–æ£€æŸ¥
try:
    import fitz  # PyMuPDF for PDF processing

    HAS_PDF_SUPPORT = True
except ImportError:
    HAS_PDF_SUPPORT = False
    print("âš ï¸ æœªå®‰è£…PyMuPDFï¼ŒPDFæ”¯æŒä¸å¯ç”¨")

try:
    from skimage import filters, morphology, segmentation, measure
    from skimage.color import rgb2hsv
    from skimage.feature import canny
    from skimage.morphology import disk, binary_erosion, binary_dilation, binary_closing, binary_opening

    HAS_SKIMAGE = True
except ImportError:
    HAS_SKIMAGE = False
    print("âš ï¸ æœªå®‰è£…scikit-imageï¼Œä½¿ç”¨åŸºç¡€å®ç°")

try:
    from sklearn.cluster import DBSCAN
    from sklearn.preprocessing import StandardScaler

    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    print("âš ï¸ æœªå®‰è£…scikit-learnï¼Œè·³è¿‡DBSCANèšç±»")

# è®¾ç½®è®¾å¤‡å’Œéšæœºç§å­
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")

torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

print("ğŸ–ï¸ å¢å¼ºç‰ˆPyTorch DQN+MCTS+CNN-LSTMæµ·å²¸çº¿æ£€æµ‹ç³»ç»Ÿ")
print("NDWI+Otsu â†’ HSVæ©è†œ â†’ Cannyè¾¹ç¼˜ â†’ DQN+MCTSæ™ºèƒ½ä¼˜åŒ– â†’ CNN-LSTMè¿ç»­æ€§ä¿®å¤")
print("=" * 90)

# æ•°æ®ç»“æ„å®šä¹‰
Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))
MCTSNode = namedtuple('MCTSNode', ('state', 'parent', 'action', 'children', 'visits', 'value', 'untried_actions'))


# ==================== åŸºç¡€å·¥å…·ç±» ====================

class BasicImageProcessor:
    """åŸºç¡€å›¾åƒå¤„ç†å™¨"""

    @staticmethod
    def rgb_to_gray(rgb_image):
        """RGBè½¬ç°åº¦"""
        if len(rgb_image.shape) == 3:
            return np.dot(rgb_image[..., :3], [0.2989, 0.5870, 0.1140])
        return rgb_image

    @staticmethod
    def gaussian_blur(image, sigma=1.0):
        """é«˜æ–¯æ¨¡ç³Š"""
        return gaussian_filter(image, sigma=sigma)

    @staticmethod
    def sobel_edges(image):
        """Sobelè¾¹ç¼˜æ£€æµ‹"""
        sobel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])
        sobel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])

        grad_x = ndimage.convolve(image, sobel_x)
        grad_y = ndimage.convolve(image, sobel_y)

        magnitude = np.sqrt(grad_x ** 2 + grad_y ** 2)
        return magnitude

    @staticmethod
    def morphology_operation(binary_image, operation='close', kernel_size=3):
        """å½¢æ€å­¦æ“ä½œ"""
        kernel = np.ones((kernel_size, kernel_size), dtype=bool)

        if operation == 'close':
            dilated = ndimage.binary_dilation(binary_image, kernel)
            return ndimage.binary_erosion(dilated, kernel)
        elif operation == 'open':
            eroded = ndimage.binary_erosion(binary_image, kernel)
            return ndimage.binary_dilation(eroded, kernel)
        elif operation == 'dilate':
            return ndimage.binary_dilation(binary_image, kernel)
        elif operation == 'erode':
            return ndimage.binary_erosion(binary_image, kernel)

        return binary_image


# ==================== ç‰¹å¾æå–ç±» ====================

class NDWIProcessor:
    """NDWIå¤„ç†å™¨"""

    def __init__(self):
        print("âœ… NDWIå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def calculate_ndwi(self, rgb_image):
        """è®¡ç®—NDWI"""
        print("ğŸŒŠ è®¡ç®—NDWI...")

        if len(rgb_image.shape) == 3:
            green = rgb_image[:, :, 1].astype(float)
            blue = rgb_image[:, :, 2].astype(float)
        else:
            green = blue = rgb_image.astype(float)

        ndwi = np.divide(green - blue, green + blue + 1e-8)
        ndwi_norm = (ndwi - ndwi.min()) / (ndwi.max() - ndwi.min() + 1e-8)

        print(f"   NDWIèŒƒå›´: [{ndwi.min():.3f}, {ndwi.max():.3f}]")
        return ndwi_norm

    def otsu_threshold(self, image):
        """Otsué˜ˆå€¼åˆ†å‰²"""
        print("ğŸ“Š Otsué˜ˆå€¼åˆ†å‰²...")

        image_int = (image * 255).astype(np.uint8)
        hist, bins = np.histogram(image_int.flatten(), 256, [0, 256])
        hist = hist.astype(float)

        total = image_int.size
        current_max = 0
        threshold = 0

        sum_total = np.sum(np.arange(256) * hist)
        sum_foreground = 0
        weight_background = 0

        for i in range(256):
            weight_background += hist[i]
            if weight_background == 0:
                continue

            weight_foreground = total - weight_background
            if weight_foreground == 0:
                break

            sum_foreground += i * hist[i]

            mean_background = sum_foreground / weight_background
            mean_foreground = (sum_total - sum_foreground) / weight_foreground

            variance_between = weight_background * weight_foreground * (mean_background - mean_foreground) ** 2

            if variance_between > current_max:
                current_max = variance_between
                threshold = i

        water_mask = (image_int > threshold).astype(float)
        threshold_norm = threshold / 255.0

        print(f"   Otsué˜ˆå€¼: {threshold_norm:.3f}")
        print(f"   æ°´ä½“åƒç´ : {np.sum(water_mask):,}")

        return water_mask, threshold_norm

    def generate_initial_mask(self, rgb_image):
        """ç”Ÿæˆåˆå§‹æ©è†œ"""
        print("\nğŸ¯ ç”ŸæˆNDWI+Otsuåˆå§‹æ©è†œ...")

        ndwi = self.calculate_ndwi(rgb_image)
        water_mask, threshold = self.otsu_threshold(ndwi)

        water_mask_cleaned = BasicImageProcessor.morphology_operation(water_mask.astype(bool), 'close', 3)
        water_mask_cleaned = BasicImageProcessor.morphology_operation(water_mask_cleaned, 'open', 2)

        return {
            'ndwi': ndwi,
            'water_mask': water_mask_cleaned.astype(float),
            'threshold': threshold,
            'raw_water_mask': water_mask
        }


class HSVOceanMaskGenerator:
    """HSVæµ·åŸŸæ©è†œç”Ÿæˆå™¨"""

    def __init__(self):
        print("âœ… HSVæµ·åŸŸæ©è†œç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")

    def rgb_to_hsv_basic(self, rgb_image):
        """åŸºç¡€RGBè½¬HSVå®ç°"""
        rgb_normalized = rgb_image.astype(float) / 255.0
        r, g, b = rgb_normalized[:, :, 0], rgb_normalized[:, :, 1], rgb_normalized[:, :, 2]

        max_val = np.maximum(np.maximum(r, g), b)
        min_val = np.minimum(np.minimum(r, g), b)
        delta = max_val - min_val

        # è‰²ç›¸ (Hue)
        h = np.zeros_like(max_val)
        mask = delta != 0

        red_mask = (max_val == r) & mask
        h[red_mask] = ((g[red_mask] - b[red_mask]) / delta[red_mask]) % 6

        green_mask = (max_val == g) & mask
        h[green_mask] = (b[green_mask] - r[green_mask]) / delta[green_mask] + 2

        blue_mask = (max_val == b) & mask
        h[blue_mask] = (r[blue_mask] - g[blue_mask]) / delta[blue_mask] + 4

        h = h * 60  # è½¬æ¢ä¸ºåº¦

        # é¥±å’Œåº¦å’Œæ˜åº¦
        s = np.zeros_like(max_val)
        s[max_val != 0] = delta[max_val != 0] / max_val[max_val != 0]
        v = max_val

        return h, s, v

    def rgb_to_hsv_mask(self, rgb_image):
        """ç”ŸæˆHSVæµ·åŸŸæ©è†œ"""
        print("ğŸŒˆ RGBâ†’HSVæµ·åŸŸæ©è†œç”Ÿæˆ...")

        if HAS_SKIMAGE:
            rgb_normalized = rgb_image.astype(float) / 255.0
            hsv = rgb2hsv(rgb_normalized)
            h, s, v = hsv[:, :, 0] * 360, hsv[:, :, 1], hsv[:, :, 2]
        else:
            h, s, v = self.rgb_to_hsv_basic(rgb_image)

        # è“è‰²èŒƒå›´
        blue_hue_mask = ((h >= 180) & (h <= 260)) | ((h >= 160) & (h <= 280))

        # é¥±å’Œåº¦å’Œäº®åº¦çº¦æŸ
        saturation_mask = s > 0.12
        brightness_mask = v > 0.15

        ocean_mask = blue_hue_mask & saturation_mask & brightness_mask

        # å½¢æ€å­¦å¤„ç†
        ocean_mask_cleaned = BasicImageProcessor.morphology_operation(ocean_mask, 'close', 4)
        ocean_mask_cleaned = BasicImageProcessor.morphology_operation(ocean_mask_cleaned, 'open', 2)

        print(f"   HSVæµ·åŸŸåƒç´ : {np.sum(ocean_mask_cleaned):,}")

        return {
            'hsv_image': np.stack([h / 360, s, v], axis=2),
            'ocean_mask': ocean_mask_cleaned.astype(float),
            'hue': h,
            'saturation': s,
            'value': v
        }


class CannyProcessor:
    """Cannyè¾¹ç¼˜æ£€æµ‹å¤„ç†å™¨"""

    def __init__(self):
        print("âœ… Cannyå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def basic_canny(self, image, low_threshold=0.1, high_threshold=0.2):
        """åŸºç¡€Cannyå®ç°"""
        if len(image.shape) == 3:
            gray = BasicImageProcessor.rgb_to_gray(image)
        else:
            gray = image

        blurred = BasicImageProcessor.gaussian_blur(gray, sigma=1.0)
        gradient = BasicImageProcessor.sobel_edges(blurred)
        gradient_norm = gradient / (gradient.max() + 1e-8)

        high_mask = gradient_norm > high_threshold
        low_mask = gradient_norm > low_threshold

        edges = high_mask.astype(float)

        for _ in range(3):
            dilated_strong = ndimage.binary_dilation(high_mask)
            connected_weak = low_mask & dilated_strong
            edges = edges | connected_weak.astype(float)
            high_mask = edges > 0.5

        return edges

    def adaptive_canny(self, image, sigma=0.33):
        """è‡ªé€‚åº”Cannyè¾¹ç¼˜æ£€æµ‹"""
        print("ğŸ” è‡ªé€‚åº”Cannyè¾¹ç¼˜æ£€æµ‹...")

        if HAS_SKIMAGE:
            if len(image.shape) == 3:
                gray = np.dot(image[..., :3], [0.2989, 0.5870, 0.1140])
            else:
                gray = image

            gray_norm = gray.astype(float) / 255.0
            median_val = np.median(gray_norm)
            lower = max(0.0, (1.0 - sigma) * median_val)
            upper = min(1.0, (1.0 + sigma) * median_val)

            edges = canny(gray_norm, sigma=1.0, low_threshold=lower, high_threshold=upper)
            edges = edges.astype(float)
        else:
            gray_norm = image.astype(float) / 255.0
            median_val = np.median(gray_norm)
            lower = max(0.0, (1.0 - sigma) * median_val)
            upper = min(1.0, (1.0 + sigma) * median_val)

            edges = self.basic_canny(image, lower, upper)

        print(f"   Cannyé˜ˆå€¼: [{lower:.3f}, {upper:.3f}]")
        print(f"   è¾¹ç¼˜åƒç´ : {np.sum(edges > 0.5):,}")

        return edges


# ==================== ç¥ç»ç½‘ç»œæ¨¡å‹ ====================

class CNNLSTMPatternMemory(nn.Module):
    """CNN-LSTMæµ·å²¸çº¿æ¨¡å¼è®°å¿†ç½‘ç»œ"""

    def __init__(self, input_channels=5, cnn_features=64, lstm_hidden=128, sequence_length=32):
        super(CNNLSTMPatternMemory, self).__init__()

        self.sequence_length = sequence_length
        self.lstm_hidden = lstm_hidden

        # CNNç‰¹å¾æå–
        self.cnn_extractor = nn.Sequential(
            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((8, 8))
        )

        # LSTMåºåˆ—å»ºæ¨¡
        self.lstm = nn.LSTM(
            input_size=64 * 8 * 8,
            hidden_size=lstm_hidden,
            num_layers=2,
            batch_first=True,
            dropout=0.3
        )

        # é¢„æµ‹å¤´
        self.predictor = nn.Sequential(
            nn.Linear(lstm_hidden, 64),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(inplace=True),
            nn.Linear(32, 2)  # é¢„æµ‹ä¸‹ä¸€ä¸ªç‚¹çš„åæ ‡
        )

        print("âœ… CNN-LSTMæ¨¡å¼è®°å¿†ç½‘ç»œåˆå§‹åŒ–å®Œæˆ")
        print(f"   åºåˆ—é•¿åº¦: {sequence_length}")
        print(f"   LSTMéšè—ç»´åº¦: {lstm_hidden}")

    def forward(self, image_patches, coastline_sequence):
        """
        Args:
            image_patches: [batch, seq_len, channels, height, width]
            coastline_sequence: [batch, seq_len, 2] æµ·å²¸çº¿ç‚¹åºåˆ—
        Returns:
            predicted_next: [batch, 2] é¢„æµ‹çš„ä¸‹ä¸€ä¸ªç‚¹
        """
        batch_size, seq_len = image_patches.shape[:2]

        # CNNç‰¹å¾æå–
        patches_flat = image_patches.view(-1, *image_patches.shape[2:])
        cnn_features = self.cnn_extractor(patches_flat)  # [batch*seq_len, 64, 8, 8]
        cnn_features = cnn_features.view(batch_size, seq_len, -1)  # [batch, seq_len, 4096]

        # LSTMåºåˆ—å»ºæ¨¡
        lstm_out, (hidden, cell) = self.lstm(cnn_features)  # [batch, seq_len, lstm_hidden]

        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºè¿›è¡Œé¢„æµ‹
        last_output = lstm_out[:, -1, :]  # [batch, lstm_hidden]

        # é¢„æµ‹ä¸‹ä¸€ä¸ªç‚¹
        predicted_next = self.predictor(last_output)  # [batch, 2]

        return predicted_next

    def extract_coastline_sequence(self, coastline_mask, sequence_length=32):
        """ä»æµ·å²¸çº¿æ©è†œæå–åºåˆ—"""
        coastline_points = np.where(coastline_mask > 0.5)

        if len(coastline_points[0]) < 2:
            return None

        # è·å–æµ·å²¸çº¿ç‚¹
        points = list(zip(coastline_points[0], coastline_points[1]))

        if len(points) < sequence_length:
            # é‡å¤ç‚¹ä»¥è¾¾åˆ°æ‰€éœ€é•¿åº¦
            points = points * (sequence_length // len(points) + 1)

        # é€‰æ‹©åºåˆ—
        sequence = points[:sequence_length]

        return np.array(sequence, dtype=np.float32)


class DQNNetwork(nn.Module):
    """å¢å¼ºç‰ˆDeep Q-Network"""

    def __init__(self, input_channels=5, hidden_dim=128, action_dim=8):
        super(DQNNetwork, self).__init__()

        # å¢å¼ºçš„å·ç§¯ç‰¹å¾æå–å™¨
        self.feature_extractor = nn.Sequential(
            # ç¬¬ä¸€å±‚å·ç§¯å—
            nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            # ç¬¬äºŒå±‚å·ç§¯å—
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            # ç¬¬ä¸‰å±‚å·ç§¯å—
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            # å…¨å±€ç‰¹å¾
            nn.AdaptiveAvgPool2d((8, 8)),
        )

        self.feature_dim = 128 * 8 * 8

        # å¢å¼ºçš„Qå€¼ç½‘ç»œ
        self.q_network = nn.Sequential(
            nn.Linear(self.feature_dim + 2, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),

            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),

            nn.Linear(hidden_dim, 64),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),

            nn.Linear(64, action_dim)
        )

    def forward(self, image_state, position):
        features = self.feature_extractor(image_state)
        features = features.view(features.size(0), -1)

        position_norm = position.float() / 400.0
        combined = torch.cat([features, position_norm], dim=1)

        q_values = self.q_network(combined)
        return q_values


# ==================== MCTSèŠ‚ç‚¹å’Œæœç´¢ ====================

class MCTSNodeClass:
    """è’™ç‰¹å¡æ´›æ ‘æœç´¢èŠ‚ç‚¹"""

    def __init__(self, state, parent=None, action=None):
        self.state = state  # å½“å‰çŠ¶æ€ï¼ˆä½ç½®ï¼‰
        self.parent = parent
        self.action = action  # å¯¼è‡´æ­¤çŠ¶æ€çš„åŠ¨ä½œ
        self.children = {}
        self.visits = 0
        self.value = 0.0
        self.untried_actions = list(range(8))  # 8ä¸ªæ–¹å‘åŠ¨ä½œ

    def is_fully_expanded(self):
        """æ˜¯å¦å®Œå…¨å±•å¼€"""
        return len(self.untried_actions) == 0

    def best_child(self, c_param=1.414):
        """é€‰æ‹©æœ€ä½³å­èŠ‚ç‚¹ï¼ˆUCB1ï¼‰"""
        choices_weights = []
        for child in self.children.values():
            if child.visits == 0:
                weight = float('inf')
            else:
                weight = (child.value / child.visits) + c_param * math.sqrt(
                    (2 * math.log(self.visits) / child.visits))
            choices_weights.append(weight)

        if not choices_weights:
            return None

        max_idx = choices_weights.index(max(choices_weights))
        best_action = list(self.children.keys())[max_idx]
        return self.children[best_action]

    def expand(self, action, next_state):
        """å±•å¼€èŠ‚ç‚¹"""
        if action in self.untried_actions:
            self.untried_actions.remove(action)
            child = MCTSNodeClass(next_state, parent=self, action=action)
            self.children[action] = child
            return child
        return None

    def update(self, reward):
        """æ›´æ–°èŠ‚ç‚¹å€¼"""
        self.visits += 1
        self.value += reward

    def backpropagate(self, reward):
        """åå‘ä¼ æ’­"""
        self.update(reward)
        if self.parent:
            self.parent.backpropagate(reward)


class MonteCarloTreeSearch:
    """è’™ç‰¹å¡æ´›æ ‘æœç´¢"""

    def __init__(self, env, iterations=100):
        self.env = env
        self.iterations = iterations

    def search(self, root_state):
        """æ‰§è¡ŒMCTSæœç´¢"""
        root = MCTSNodeClass(root_state)

        for _ in range(self.iterations):
            # é€‰æ‹©
            node = self._select(root)

            # å±•å¼€
            if not node.is_fully_expanded() and node.untried_actions:
                action = random.choice(node.untried_actions)
                next_state, reward = self.env.step(node.state, action)
                child = node.expand(action, next_state)
                if child:
                    node = child

            # æ¨¡æ‹Ÿ
            simulation_reward = self._simulate(node.state)

            # åå‘ä¼ æ’­
            node.backpropagate(simulation_reward)

        # è¿”å›æœ€ä½³åŠ¨ä½œ
        if root.children:
            best_child = root.best_child(c_param=0)  # åˆ©ç”¨é˜¶æ®µï¼Œä¸æ¢ç´¢
            if best_child:
                return best_child.action

        return random.randrange(8)  # éšæœºåŠ¨ä½œ

    def _select(self, node):
        """é€‰æ‹©é˜¶æ®µ"""
        while node.is_fully_expanded() and node.children:
            node = node.best_child()
        return node

    def _simulate(self, state, max_depth=10):
        """æ¨¡æ‹Ÿé˜¶æ®µ"""
        current_state = state
        total_reward = 0.0

        for _ in range(max_depth):
            action = random.randrange(8)
            next_state, reward = self.env.step(current_state, action)
            total_reward += reward
            current_state = next_state

            if reward < -30:  # ç»ˆæ­¢æ¡ä»¶
                break

        return total_reward / max_depth


# ==================== ç»éªŒå›æ”¾å’Œç¯å¢ƒ ====================

class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""

    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, next_state, reward):
        self.buffer.append(Transition(state, action, next_state, reward))

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)

    def __len__(self):
        return len(self.buffer)


class EnhancedDQNCoastlineEnvironment:
    """å¢å¼ºç‰ˆDQNæµ·å²¸çº¿ä¼˜åŒ–ç¯å¢ƒ"""

    def __init__(self, image, ocean_mask, initial_coastline):
        self.image = image
        self.ocean_mask = ocean_mask
        self.current_coastline = initial_coastline.copy()
        self.initial_coastline = initial_coastline.copy()
        self.height, self.width = image.shape[:2]

        # åŠ¨ä½œç©ºé—´ï¼š8ä¸ªæ–¹å‘ç§»åŠ¨
        self.actions = [(-1, -1), (-1, 0), (-1, 1), (0, -1),
                        (0, 1), (1, -1), (1, 0), (1, 1)]
        self.action_dim = len(self.actions)

        # å®šä¹‰æœç´¢åŒºåŸŸ
        self.search_region = self._define_search_region()

        # æå–è¾¹ç¼˜åŒºåŸŸç”¨äºMCTS
        self.edge_region = self._extract_edge_region()

        print(f"âœ… å¢å¼ºç‰ˆDQNç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"   å›¾åƒå°ºå¯¸: {self.height}x{self.width}")
        print(f"   æœç´¢åŒºåŸŸ: {np.sum(self.search_region):,} åƒç´ ")
        print(f"   è¾¹ç¼˜åŒºåŸŸ: {np.sum(self.edge_region):,} åƒç´ ")

    def _define_search_region(self):
        """å®šä¹‰æœç´¢åŒºåŸŸ"""
        coastline_binary = (self.initial_coastline > 0.5).astype(bool)

        search_region = coastline_binary.copy()
        for _ in range(15):  # æ‰©å¤§æœç´¢èŒƒå›´
            search_region = BasicImageProcessor.morphology_operation(search_region, 'dilate', 3)

        return search_region

    def _extract_edge_region(self):
        """æå–æµ·å²¸çº¿è¾¹ç¼˜åŒºåŸŸï¼ˆç”¨äºMCTSï¼‰"""
        coastline_binary = (self.initial_coastline > 0.5).astype(bool)

        # è†¨èƒ€åå‡å»åŸå›¾å¾—åˆ°è¾¹ç¼˜
        dilated = BasicImageProcessor.morphology_operation(coastline_binary, 'dilate', 5)
        edge_region = dilated & ~coastline_binary

        return edge_region

    def get_state_tensor(self, position):
        """è·å–çŠ¶æ€å¼ é‡"""
        y, x = position

        window_size = 64
        half_window = window_size // 2

        y_start = max(0, y - half_window)
        y_end = min(self.height, y + half_window)
        x_start = max(0, x - half_window)
        x_end = min(self.width, x + half_window)

        state = np.zeros((5, window_size, window_size), dtype=np.float32)

        actual_h = y_end - y_start
        actual_w = x_end - x_start

        # RGBé€šé“
        if len(self.image.shape) == 3:
            rgb_window = self.image[y_start:y_end, x_start:x_end] / 255.0
            state[0:3, :actual_h, :actual_w] = rgb_window.transpose(2, 0, 1)
        else:
            gray_window = self.image[y_start:y_end, x_start:x_end] / 255.0
            state[0:3, :actual_h, :actual_w] = gray_window

        # æµ·åŸŸæ©è†œé€šé“
        ocean_window = self.ocean_mask[y_start:y_end, x_start:x_end]
        state[3, :actual_h, :actual_w] = ocean_window

        # å½“å‰æµ·å²¸çº¿é€šé“
        coastline_window = self.current_coastline[y_start:y_end, x_start:x_end]
        state[4, :actual_h, :actual_w] = coastline_window

        return torch.FloatTensor(state).unsqueeze(0).to(device)

    def step(self, position, action_idx):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        y, x = position
        dy, dx = self.actions[action_idx]

        new_y = np.clip(y + dy, 0, self.height - 1)
        new_x = np.clip(x + dx, 0, self.width - 1)

        new_position = (new_y, new_x)
        reward = self._calculate_enhanced_reward(position, new_position, action_idx)

        return new_position, reward

    def _calculate_enhanced_reward(self, old_pos, new_pos, action_idx):
        """å¢å¼ºç‰ˆå¥–åŠ±å‡½æ•°"""
        y, x = new_pos
        reward = 0.0

        # 1. è¾¹ç•Œæ£€æŸ¥
        if not (0 <= y < self.height and 0 <= x < self.width):
            return -15.0

        # 2. æœç´¢åŒºåŸŸå†…å¥–åŠ±
        if self.search_region[y, x]:
            reward += 3.0
        else:
            reward -= 2.0

        # 3. è¾¹ç¼˜åŒºåŸŸå¥–åŠ±ï¼ˆMCTSé‡ç‚¹å…³æ³¨ï¼‰
        if self.edge_region[y, x]:
            reward += 5.0

        # 4. æµ·åŸŸå†…éƒ¨å·¨å¤§æƒ©ç½š
        if self.ocean_mask[y, x] > 0.5:
            if self._is_deep_ocean(y, x):
                reward -= 100.0  # åŠ é‡æƒ©ç½š
                return reward

        # 5. è¾¹ç¼˜å¼ºåº¦å¥–åŠ±
        edge_reward = self._calculate_edge_reward(y, x)
        reward += edge_reward * 4.0

        # 6. è¿ç»­æ€§å¥–åŠ±ï¼ˆåŠ å¼ºï¼‰
        continuity_reward = self._calculate_continuity_reward(y, x)
        reward += continuity_reward * 3.0

        # 7. è·ç¦»é€‚å½“æ€§å¥–åŠ±
        distance_reward = self._calculate_distance_reward(y, x)
        reward += distance_reward * 2.0

        # 8. æ–°å¢ï¼šå±€éƒ¨è¿é€šæ€§å¥–åŠ±
        connectivity_reward = self._calculate_connectivity_reward(y, x)
        reward += connectivity_reward * 2.0

        return reward

    def _is_deep_ocean(self, y, x, erosion_depth=8):
        """æ£€æŸ¥æ˜¯å¦åœ¨æ·±æµ·ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        ocean_binary = (self.ocean_mask > 0.5).astype(bool)

        deep_ocean = ocean_binary.copy()
        for _ in range(erosion_depth):
            deep_ocean = BasicImageProcessor.morphology_operation(deep_ocean, 'erode', 3)

        return deep_ocean[y, x] if 0 <= y < self.height and 0 <= x < self.width else False

    def _calculate_edge_reward(self, y, x):
        """è®¡ç®—è¾¹ç¼˜å¼ºåº¦å¥–åŠ±"""
        if not (1 <= y < self.height - 1 and 1 <= x < self.width - 1):
            return 0.0

        gray = BasicImageProcessor.rgb_to_gray(self.image) if len(self.image.shape) == 3 else self.image

        gx = (gray[y, x + 1] - gray[y, x - 1]) / 2.0
        gy = (gray[y + 1, x] - gray[y - 1, x]) / 2.0

        gradient_magnitude = math.sqrt(gx * gx + gy * gy)
        return min(gradient_magnitude / 40.0, 1.0)

    def _calculate_continuity_reward(self, y, x):
        """è®¡ç®—è¿ç»­æ€§å¥–åŠ±ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        neighbors = 0
        neighbor_positions = []

        for dy in [-1, 0, 1]:
            for dx in [-1, 0, 1]:
                if dy == 0 and dx == 0:
                    continue
                ny, nx = y + dy, x + dx
                if (0 <= ny < self.height and 0 <= nx < self.width and
                        self.current_coastline[ny, nx] > 0.5):
                    neighbors += 1
                    neighbor_positions.append((ny, nx))

        # ç†æƒ³é‚»å±…æ•°ä¸º2ï¼ˆè¿ç»­çº¿ï¼‰
        if neighbors == 2:
            # æ£€æŸ¥æ˜¯å¦å½¢æˆç›´çº¿æˆ–åˆç†å¼¯æ›²
            if len(neighbor_positions) == 2:
                p1, p2 = neighbor_positions
                # è®¡ç®—è§’åº¦ï¼Œå¥–åŠ±å¹³æ»‘è¿æ¥
                angle_reward = self._calculate_angle_reward((y, x), p1, p2)
                return 1.5 + angle_reward
            return 1.5
        elif neighbors == 1:
            return 1.0
        elif neighbors == 3:
            return 0.7
        else:
            return 0.2

    def _calculate_angle_reward(self, center, p1, p2):
        """è®¡ç®—è§’åº¦å¥–åŠ±"""
        v1 = (p1[0] - center[0], p1[1] - center[1])
        v2 = (p2[0] - center[0], p2[1] - center[1])

        dot_product = v1[0] * v2[0] + v1[1] * v2[1]
        magnitude = math.sqrt(v1[0] ** 2 + v1[1] ** 2) * math.sqrt(v2[0] ** 2 + v2[1] ** 2)

        if magnitude == 0:
            return 0.0

        cos_angle = dot_product / magnitude
        cos_angle = max(-1.0, min(1.0, cos_angle))  # é™åˆ¶èŒƒå›´

        # å¥–åŠ±å¹³æ»‘çš„è§’åº¦ï¼ˆæ¥è¿‘180åº¦æˆ–å¹³ç›´ï¼‰
        angle = math.acos(abs(cos_angle))
        smoothness = 1.0 - (angle / math.pi) * 2.0
        return max(0.0, smoothness * 0.5)

    def _calculate_distance_reward(self, y, x):
        """è®¡ç®—è·ç¦»é€‚å½“æ€§å¥–åŠ±"""
        coastline_points = np.where(self.current_coastline > 0.5)
        if len(coastline_points[0]) == 0:
            return 0.0

        distances = np.sqrt((coastline_points[0] - y) ** 2 + (coastline_points[1] - x) ** 2)
        min_distance = np.min(distances)

        if 2 <= min_distance <= 5:
            return 1.0
        elif 1 <= min_distance <= 8:
            return 0.7
        else:
            return 0.3

    def _calculate_connectivity_reward(self, y, x):
        """è®¡ç®—å±€éƒ¨è¿é€šæ€§å¥–åŠ±"""
        # æ£€æŸ¥å‘¨å›´3x3åŒºåŸŸçš„è¿é€šæ€§
        local_region = self.current_coastline[max(0, y - 3):min(self.height, y + 4),
                       max(0, x - 3):min(self.width, x + 4)]

        if local_region.size == 0:
            return 0.0

        # è®¡ç®—å±€éƒ¨å¯†åº¦
        density = np.sum(local_region > 0.5) / local_region.size

        # ç†æƒ³å¯†åº¦åœ¨0.1-0.3ä¹‹é—´
        if 0.1 <= density <= 0.3:
            return 1.0
        elif 0.05 <= density <= 0.5:
            return 0.6
        else:
            return 0.2

    def update_coastline(self, position, value=1.0):
        """æ›´æ–°æµ·å²¸çº¿"""
        y, x = position
        if 0 <= y < self.height and 0 <= x < self.width:
            self.current_coastline[y, x] = min(1.0, self.current_coastline[y, x] + value)


# ==================== DQNæ™ºèƒ½ä»£ç† ====================

class EnhancedDQNAgent:
    """å¢å¼ºç‰ˆDQNæ™ºèƒ½ä»£ç†ï¼ˆç»“åˆMCTSï¼‰"""

    def __init__(self, env, lr=1e-4, gamma=0.99, epsilon_start=1.0, epsilon_end=0.05, epsilon_decay=0.995):
        self.env = env
        self.device = device

        # è¶…å‚æ•°
        self.lr = lr
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay

        # ç½‘ç»œ
        self.policy_net = DQNNetwork().to(device)
        self.target_net = DQNNetwork().to(device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr, weight_decay=1e-5)

        # ç»éªŒå›æ”¾
        self.memory = ReplayBuffer(capacity=8000)

        # MCTS
        self.mcts = MonteCarloTreeSearch(env, iterations=50)

        # è®­ç»ƒå‚æ•°
        self.batch_size = 32
        self.target_update_freq = 150
        self.train_freq = 4
        self.steps_done = 0

        print(f"âœ… å¢å¼ºç‰ˆDQNä»£ç†åˆå§‹åŒ–å®Œæˆ")
        print(f"   MCTSè¿­ä»£æ¬¡æ•°: 50")
        print(f"   å­¦ä¹ ç‡: {lr}")

    def select_action(self, state, position, training=True, use_mcts=False):
        """é€‰æ‹©åŠ¨ä½œï¼ˆDQN + MCTSï¼‰"""
        if training and random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            return random.randrange(self.env.action_dim)
        elif use_mcts and self.env.edge_region[position[0], position[1]]:
            # åœ¨è¾¹ç¼˜åŒºåŸŸä½¿ç”¨MCTS
            return self.mcts.search(position)
        else:
            # åˆ©ç”¨ï¼šä½¿ç”¨DQNé€‰æ‹©åŠ¨ä½œ
            with torch.no_grad():
                position_tensor = torch.LongTensor([position]).to(device)
                q_values = self.policy_net(state, position_tensor)
                return q_values.argmax(dim=1).item()

    def train_step(self):
        """è®­ç»ƒæ­¥éª¤"""
        if len(self.memory) < self.batch_size:
            return None

        transitions = self.memory.sample(self.batch_size)
        batch = Transition(*zip(*transitions))

        state_batch = torch.cat([t[0] for t in batch.state])
        position_batch = torch.LongTensor([t[1] for t in batch.state]).to(device)
        action_batch = torch.LongTensor(batch.action).to(device)
        reward_batch = torch.FloatTensor(batch.reward).to(device)

        current_q_values = self.policy_net(state_batch, position_batch).gather(1, action_batch.unsqueeze(1))

        next_state_values = torch.zeros(self.batch_size).to(device)
        non_final_mask = torch.tensor([s is not None for s in batch.next_state], dtype=torch.bool).to(device)

        if non_final_mask.any():
            non_final_next_states = torch.cat([t[0] for t in batch.next_state if t is not None])
            non_final_next_positions = torch.LongTensor([t[1] for t in batch.next_state if t is not None]).to(device)

            with torch.no_grad():
                next_state_values[non_final_mask] = self.target_net(
                    non_final_next_states, non_final_next_positions
                ).max(1)[0]

        target_q_values = reward_batch + (self.gamma * next_state_values)

        # HuberæŸå¤±
        loss = F.smooth_l1_loss(current_q_values.squeeze(), target_q_values)

        self.optimizer.zero_grad()
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)

        self.optimizer.step()

        return loss.item()

    def update_target_network(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_net.load_state_dict(self.policy_net.state_dict())

    def decay_epsilon(self):
        """è¡°å‡æ¢ç´¢ç‡"""
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)

    def optimize_coastline(self, max_episodes=120, max_steps_per_episode=200):
        """ä¼˜åŒ–æµ·å²¸çº¿ï¼ˆDQN + MCTSï¼‰"""
        print("ğŸ¯ DQN+MCTSæµ·å²¸çº¿ä¼˜åŒ–å¼€å§‹...")

        search_positions = np.where(self.env.search_region)
        candidate_positions = list(zip(search_positions[0], search_positions[1]))

        if not candidate_positions:
            print("   âš ï¸ æœªæ‰¾åˆ°æœç´¢åŒºåŸŸ")
            return self.env.current_coastline

        episode_rewards = []
        improvements_made = 0
        mcts_usage = 0

        for episode in range(max_episodes):
            start_position = random.choice(candidate_positions)
            current_position = start_position

            episode_reward = 0

            for step in range(max_steps_per_episode):
                state = self.env.get_state_tensor(current_position)

                # åœ¨è¾¹ç¼˜åŒºåŸŸä½¿ç”¨MCTSï¼Œå…¶ä»–åœ°æ–¹ä½¿ç”¨DQN
                use_mcts = (episode > 20 and  # é¢„çƒ­åä½¿ç”¨MCTS
                            self.env.edge_region[current_position[0], current_position[1]])

                if use_mcts:
                    mcts_usage += 1

                action = self.select_action(state, current_position, training=True, use_mcts=use_mcts)
                next_position, reward = self.env.step(current_position, action)
                episode_reward += reward

                next_state = self.env.get_state_tensor(next_position)

                self.memory.push(
                    (state, current_position),
                    action,
                    (next_state, next_position) if reward > -50 else None,
                    reward
                )

                # æ›´æ–°æµ·å²¸çº¿
                if reward > 3.0:
                    self.env.update_coastline(next_position, 0.4)
                    improvements_made += 1

                # è®­ç»ƒ
                if self.steps_done % self.train_freq == 0:
                    loss = self.train_step()

                # æ›´æ–°ç›®æ ‡ç½‘ç»œ
                if self.steps_done % self.target_update_freq == 0:
                    self.update_target_network()

                self.steps_done += 1
                current_position = next_position

                if reward < -50:  # æ—©åœ
                    break

            episode_rewards.append(episode_reward)
            self.decay_epsilon()

            if episode % 30 == 0:
                avg_reward = np.mean(episode_rewards[-30:])
                print(f"   Episode {episode:3d}: å¹³å‡å¥–åŠ±={avg_reward:6.2f}, Îµ={self.epsilon:.3f}, "
                      f"æ”¹è¿›={improvements_made}, MCTSä½¿ç”¨={mcts_usage}")

        print(f"   âœ… DQN+MCTSä¼˜åŒ–å®Œæˆ")
        print(f"   æ€»æ”¹è¿›æ¬¡æ•°: {improvements_made}")
        print(f"   MCTSä½¿ç”¨æ¬¡æ•°: {mcts_usage}")

        return self.env.current_coastline


# ==================== è¿æ¥ä¿®å¤å™¨ ====================

class CoastlineConnectionRepair:
    """æµ·å²¸çº¿è¿æ¥ä¿®å¤å™¨"""

    def __init__(self):
        print("âœ… æµ·å²¸çº¿è¿æ¥ä¿®å¤å™¨åˆå§‹åŒ–å®Œæˆ")

    def repair_coastline_connections(self, coastline_mask, max_gap=10):
        """ä¿®å¤æµ·å²¸çº¿è¿æ¥"""
        print("ğŸ”§ ä¿®å¤æµ·å²¸çº¿è¿æ¥...")

        coastline_binary = (coastline_mask > 0.5).astype(bool)

        # 1. è¯†åˆ«è¿é€šç»„ä»¶
        labeled_array, num_components = label(coastline_binary)

        print(f"   å‘ç° {num_components} ä¸ªè¿é€šç»„ä»¶")

        if num_components <= 1:
            return coastline_mask

        # 2. æå–å„ä¸ªç»„ä»¶çš„ç«¯ç‚¹
        components_endpoints = []
        for i in range(1, num_components + 1):
            component = (labeled_array == i)
            endpoints = self._find_component_endpoints(component)
            components_endpoints.append((i, component, endpoints))

        # 3. è¿æ¥ç›¸è¿‘çš„ç»„ä»¶
        repaired_coastline = coastline_binary.astype(float)
        connections_made = 0

        for i in range(len(components_endpoints)):
            for j in range(i + 1, len(components_endpoints)):
                comp1_id, comp1_mask, endpoints1 = components_endpoints[i]
                comp2_id, comp2_mask, endpoints2 = components_endpoints[j]

                # æ‰¾åˆ°æœ€è¿‘çš„ç«¯ç‚¹å¯¹
                min_distance = float('inf')
                best_connection = None

                for ep1 in endpoints1:
                    for ep2 in endpoints2:
                        distance = math.sqrt((ep1[0] - ep2[0]) ** 2 + (ep1[1] - ep2[1]) ** 2)
                        if distance < min_distance and distance <= max_gap:
                            min_distance = distance
                            best_connection = (ep1, ep2)

                # å¦‚æœæ‰¾åˆ°åˆé€‚çš„è¿æ¥ï¼Œç»˜åˆ¶è¿æ¥çº¿
                if best_connection:
                    self._draw_connection_line(repaired_coastline, best_connection[0], best_connection[1])
                    connections_made += 1

        print(f"   å®Œæˆ {connections_made} ä¸ªè¿æ¥")

        return repaired_coastline

    def _find_component_endpoints(self, component):
        """æ‰¾åˆ°ç»„ä»¶çš„ç«¯ç‚¹"""
        coords = np.where(component)
        points = list(zip(coords[0], coords[1]))

        if len(points) < 2:
            return points

        endpoints = []

        # æ‰¾åˆ°åº¦æ•°ä¸º1çš„ç‚¹ï¼ˆç«¯ç‚¹ï¼‰
        for point in points:
            neighbors = 0
            y, x = point

            for dy in [-1, 0, 1]:
                for dx in [-1, 0, 1]:
                    if dy == 0 and dx == 0:
                        continue
                    ny, nx = y + dy, x + dx
                    if (0 <= ny < component.shape[0] and 0 <= nx < component.shape[1] and
                            component[ny, nx]):
                        neighbors += 1

            if neighbors <= 1:  # ç«¯ç‚¹æˆ–å­¤ç«‹ç‚¹
                endpoints.append(point)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜æ˜¾ç«¯ç‚¹ï¼Œé€‰æ‹©æœ€è¿œçš„ä¸¤ä¸ªç‚¹
        if len(endpoints) == 0 and len(points) >= 2:
            max_distance = 0
            for i, p1 in enumerate(points):
                for j, p2 in enumerate(points[i + 1:], i + 1):
                    distance = math.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)
                    if distance > max_distance:
                        max_distance = distance
                        endpoints = [p1, p2]

        return endpoints[:2]  # æœ€å¤šè¿”å›2ä¸ªç«¯ç‚¹

    def _draw_connection_line(self, coastline, start, end):
        """ç»˜åˆ¶è¿æ¥çº¿"""
        y1, x1 = start
        y2, x2 = end

        # Bresenhamç›´çº¿ç®—æ³•
        points = self._bresenham_line(x1, y1, x2, y2)

        for x, y in points:
            if 0 <= y < coastline.shape[0] and 0 <= x < coastline.shape[1]:
                coastline[y, x] = 1.0

    def _bresenham_line(self, x1, y1, x2, y2):
        """Bresenhamç›´çº¿ç®—æ³•"""
        points = []

        dx = abs(x2 - x1)
        dy = abs(y2 - y1)

        sx = 1 if x1 < x2 else -1
        sy = 1 if y1 < y2 else -1

        err = dx - dy

        while True:
            points.append((x1, y1))

            if x1 == x2 and y1 == y2:
                break

            e2 = 2 * err

            if e2 > -dy:
                err -= dy
                x1 += sx

            if e2 < dx:
                err += dx
                y1 += sy

        return points


# ==================== ä¸»æ£€æµ‹å™¨ç±» ====================

class EnhancedCoastlineDetector:
    """å¢å¼ºç‰ˆæµ·å²¸çº¿æ£€æµ‹ç³»ç»Ÿä¸»ç±»"""

    def __init__(self):
        self.ndwi_processor = NDWIProcessor()
        self.hsv_processor = HSVOceanMaskGenerator()
        self.canny_processor = CannyProcessor()
        self.connection_repair = CoastlineConnectionRepair()

        # CNN-LSTMæ¨¡å¼è®°å¿†ç½‘ç»œ
        self.pattern_memory = CNNLSTMPatternMemory().to(device)
        self.pattern_optimizer = optim.Adam(self.pattern_memory.parameters(), lr=1e-4)

        print("âœ… å¢å¼ºç‰ˆæµ·å²¸çº¿æ£€æµ‹ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print("   ğŸ§  DQN + MCTS + CNN-LSTM ä¸‰é‡å¢å¼º")

    def load_image_from_file(self, image_path):
        """ä»æ–‡ä»¶åŠ è½½å›¾åƒ"""
        try:
            if image_path.lower().endswith('.pdf') and HAS_PDF_SUPPORT:
                doc = fitz.open(image_path)
                page = doc.load_page(0)
                zoom = 200 / 72
                mat = fitz.Matrix(zoom, zoom)
                pix = page.get_pixmap(matrix=mat)
                img_data = pix.tobytes("png")

                img = Image.open(BytesIO(img_data))
                image_array = np.array(img)
                doc.close()

                return image_array
            else:
                img = Image.open(image_path)
                return np.array(img)

        except Exception as e:
            print(f"âŒ å›¾åƒåŠ è½½å¤±è´¥: {e}")
            return None

    def _predict_coastline_extension(self, image, component, ocean_mask):
        """ä½¿ç”¨CNN-LSTMé¢„æµ‹æµ·å²¸çº¿å»¶ä¼¸"""
        coords = np.where(component)
        if len(coords[0]) < 5:
            return None

        points = list(zip(coords[0], coords[1]))

        # ç®€åŒ–çš„æ¨¡å¼è®°å¿†ï¼šåŸºäºå±€éƒ¨æ¢¯åº¦å’Œæ–¹å‘é¢„æµ‹
        extensions = np.zeros_like(component, dtype=float)

        for point in points[-5:]:  # ä½¿ç”¨æœ€åå‡ ä¸ªç‚¹
            y, x = point

            # è®¡ç®—å±€éƒ¨æ¢¯åº¦æ–¹å‘
            if (1 <= y < image.shape[0] - 1 and 1 <= x < image.shape[1] - 1):
                gray = BasicImageProcessor.rgb_to_gray(image) if len(image.shape) == 3 else image

                gx = (gray[y, x + 1] - gray[y, x - 1]) / 2.0
                gy = (gray[y + 1, x] - gray[y - 1, x]) / 2.0

                # æ¢¯åº¦æ–¹å‘
                if abs(gx) > 1e-6 or abs(gy) > 1e-6:
                    magnitude = math.sqrt(gx * gx + gy * gy)
                    gx_norm = gx / magnitude
                    gy_norm = gy / magnitude

                    # æ²¿æ¢¯åº¦æ–¹å‘å»¶ä¼¸
                    for step in range(1, 8):
                        extend_y = int(y + gy_norm * step)
                        extend_x = int(x + gx_norm * step)

                        if (0 <= extend_y < extensions.shape[0] and
                                0 <= extend_x < extensions.shape[1] and
                                ocean_mask[extend_y, extend_x] < 0.3):  # é¿å…æµ·åŸŸå†…éƒ¨

                            extensions[extend_y, extend_x] = max(0.5,
                                                                 extensions[extend_y, extend_x])

        return extensions if np.sum(extensions > 0.5) > 0 else None

    def _fuse_initial_coastlines(self, ndwi_result, hsv_result, canny_edges, gt_coastline):
        """èåˆå¤šæºä¿¡æ¯ç”Ÿæˆåˆå§‹æµ·å²¸çº¿"""
        print("   ğŸ”„ å¤šæºä¿¡æ¯èåˆ...")

        # å¢å¼ºæƒé‡è®¾ç½®
        weights = {
            'ndwi': 0.25,
            'hsv': 0.25,
            'canny': 0.45,
            'gt': 0.05 if gt_coastline is not None else 0.0
        }

        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(weights.values())
        for key in weights:
            weights[key] /= total_weight

        # åŠ æƒèåˆ
        fused_coastline = np.zeros_like(canny_edges)

        # NDWIè¾¹ç¼˜è´¡çŒ®
        ndwi_edges = self._extract_edges_from_mask(ndwi_result['water_mask'])
        fused_coastline += weights['ndwi'] * ndwi_edges

        # HSVè¾¹ç¼˜è´¡çŒ®
        hsv_edges = self._extract_edges_from_mask(hsv_result['ocean_mask'])
        fused_coastline += weights['hsv'] * hsv_edges

        # Cannyè¾¹ç¼˜è´¡çŒ®
        fused_coastline += weights['canny'] * canny_edges

        # Ground Truthè´¡çŒ®
        if gt_coastline is not None:
            fused_coastline += weights['gt'] * gt_coastline

        # å¢å¼ºé˜ˆå€¼åŒ–
        initial_coastline = (fused_coastline > 0.25).astype(float)

        # å½¢æ€å­¦ä¼˜åŒ–
        initial_coastline = BasicImageProcessor.morphology_operation(
            initial_coastline.astype(bool), 'close', 4
        ).astype(float)
        initial_coastline = BasicImageProcessor.morphology_operation(
            initial_coastline.astype(bool), 'open', 2
        ).astype(float)

        print(f"   èåˆæµ·å²¸çº¿åƒç´ : {np.sum(initial_coastline):,}")
        return initial_coastline

    def _extract_edges_from_mask(self, mask):
        """ä»æ©è†œæå–è¾¹ç¼˜"""
        mask_binary = (mask > 0.5).astype(bool)
        dilated = BasicImageProcessor.morphology_operation(mask_binary, 'dilate', 4)
        eroded = BasicImageProcessor.morphology_operation(mask_binary, 'erode', 4)
        edges = (dilated & ~eroded).astype(float)
        return edges

    def _apply_ocean_penalty(self, coastline, ocean_mask):
        """åº”ç”¨æµ·åŸŸè½®å»“æƒ©ç½š"""
        print("   ğŸŒŠ åº”ç”¨æµ·åŸŸè½®å»“æƒ©ç½š...")

        ocean_binary = (ocean_mask > 0.5).astype(bool)

        # æ›´æ·±çš„è…èš€å¾—åˆ°æµ·åŸŸæ·±å¤„
        ocean_interior = ocean_binary.copy()
        for _ in range(8):
            ocean_interior = BasicImageProcessor.morphology_operation(ocean_interior, 'erode', 3)

        coastline_binary = (coastline > 0.5).astype(bool)
        interior_coastline_points = np.sum(coastline_binary & ocean_interior)

        if interior_coastline_points > 0:
            print(f"   âš ï¸ ç§»é™¤æµ·åŸŸå†…éƒ¨è½®å»“ç‚¹: {interior_coastline_points:,}")
            coastline_corrected = coastline.copy()
            coastline_corrected[ocean_interior] = 0
            return coastline_corrected
        else:
            print("   âœ… æ— æµ·åŸŸå†…éƒ¨è½®å»“")
            return coastline

    def _apply_pattern_memory_repair(self, image, coastline, ocean_mask):
        """åº”ç”¨CNN-LSTMæ¨¡å¼è®°å¿†ä¿®å¤"""
        print("   ğŸ§  CNN-LSTMæ¨¡å¼è®°å¿†ä¿®å¤...")

        # è¯†åˆ«éœ€è¦ä¿®å¤çš„æ–­è£‚åŒºåŸŸ
        coastline_binary = (coastline > 0.5).astype(bool)
        labeled_array, num_components = label(coastline_binary)

        if num_components <= 1:
            print("   âœ… æ— éœ€æ¨¡å¼è®°å¿†ä¿®å¤")
            return coastline

        repaired_coastline = coastline.copy()

        # ä¸ºæ¯ä¸ªå°ç»„ä»¶å°è¯•é¢„æµ‹è¿æ¥
        for comp_id in range(1, num_components + 1):
            component = (labeled_array == comp_id)
            comp_size = np.sum(component)

            # åªå¤„ç†è¾ƒå°çš„ç»„ä»¶
            if comp_size < 50:
                predicted_extension = self._predict_coastline_extension(
                    image, component, ocean_mask
                )
                if predicted_extension is not None:
                    repaired_coastline = np.maximum(repaired_coastline, predicted_extension)

        improvement = np.sum(repaired_coastline > 0.5) - np.sum(coastline > 0.5)
        if improvement > 0:
            print(f"   ğŸ“ˆ æ¨¡å¼è®°å¿†ä¿®å¤å¢åŠ  {improvement:,} ä¸ªåƒç´ ")

        return repaired_coastline

    def _evaluate_quality(self, predicted, ground_truth, ocean_mask):
        """è¯„ä¼°æµ·å²¸çº¿è´¨é‡ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        metrics = {}

        # åŸºç¡€ç»Ÿè®¡
        pred_binary = (predicted > 0.5).astype(bool)
        coastline_pixels = np.sum(pred_binary)
        total_pixels = predicted.size

        metrics['coastline_pixels'] = int(coastline_pixels)
        metrics['coverage_ratio'] = float(coastline_pixels / total_pixels)

        # è¿é€šæ€§åˆ†æ
        labeled_array, num_components = label(pred_binary)
        metrics['num_components'] = int(num_components)

        # è¿ç»­æ€§è¯„ä¼°
        continuity_score = max(0.0, 1.0 - (num_components - 1) * 0.15)
        metrics['continuity_score'] = float(continuity_score)

        # æµ·åŸŸæƒ©ç½šæ£€æŸ¥
        ocean_binary = (ocean_mask > 0.5).astype(bool)
        ocean_coastline_pixels = np.sum(pred_binary & ocean_binary)
        metrics['ocean_penalty_pixels'] = int(ocean_coastline_pixels)
        metrics['ocean_penalty_ratio'] = float(ocean_coastline_pixels / max(coastline_pixels, 1))

        # Ground Truthå‡†ç¡®æ€§æŒ‡æ ‡
        if ground_truth is not None:
            gt_binary = (ground_truth > 0.5).astype(bool)

            tp = np.sum(pred_binary & gt_binary)
            fp = np.sum(pred_binary & ~gt_binary)
            fn = np.sum(~pred_binary & gt_binary)

            precision = tp / (tp + fp + 1e-8)
            recall = tp / (tp + fn + 1e-8)
            f1_score = 2 * precision * recall / (precision + recall + 1e-8)
            iou = tp / (tp + fp + fn + 1e-8)

            metrics['precision'] = float(precision)
            metrics['recall'] = float(recall)
            metrics['f1_score'] = float(f1_score)
            metrics['iou'] = float(iou)

        # ç»¼åˆè´¨é‡å¾—åˆ†ï¼ˆå¢å¼ºç‰ˆï¼‰
        base_score = min(1.0, coastline_pixels / 800.0)  # å¢åŠ åŸºç¡€åˆ†æ•°è¦æ±‚
        penalty_score = max(0.0, 1.0 - metrics['ocean_penalty_ratio'] * 3.0)  # åŠ é‡æƒ©ç½š
        continuity_score = metrics['continuity_score']

        # é•¿åº¦è´¨é‡è¯„ä¼°
        length_quality = self._evaluate_coastline_length_quality(pred_binary)
        metrics['length_quality'] = float(length_quality)

        overall_score = base_score * penalty_score * continuity_score * length_quality

        # å¦‚æœæœ‰Ground Truthï¼Œç»“åˆå‡†ç¡®æ€§
        if ground_truth is not None and 'f1_score' in metrics:
            overall_score = (overall_score * 0.7 + metrics['f1_score'] * 0.3)

        metrics['overall_score'] = float(overall_score)

        return metrics

    def _evaluate_coastline_length_quality(self, coastline_binary):
        """è¯„ä¼°æµ·å²¸çº¿é•¿åº¦è´¨é‡"""
        if not np.any(coastline_binary):
            return 0.0

        # è®¡ç®—æµ·å²¸çº¿çš„å®é™…é•¿åº¦ï¼ˆè€ƒè™‘8è¿é€šï¼‰
        coords = np.where(coastline_binary)
        points = list(zip(coords[0], coords[1]))

        if len(points) < 2:
            return 0.1

        # ç®€å•çš„é•¿åº¦ä¼°ç®—
        total_length = 0
        for i, point in enumerate(points):
            neighbors = 0
            y, x = point

            for dy in [-1, 0, 1]:
                for dx in [-1, 0, 1]:
                    if dy == 0 and dx == 0:
                        continue
                    ny, nx = y + dy, x + dx
                    if (0 <= ny < coastline_binary.shape[0] and
                            0 <= nx < coastline_binary.shape[1] and
                            coastline_binary[ny, nx]):
                        neighbors += 1
                        if abs(dy) + abs(dx) == 2:  # å¯¹è§’çº¿
                            total_length += 1.414
                        else:
                            total_length += 1.0

        # æ ‡å‡†åŒ–é•¿åº¦è´¨é‡
        expected_length = len(points) * 1.5  # æœŸæœ›å¹³å‡è¿æ¥åº¦
        length_ratio = min(1.0, total_length / max(expected_length, 1))

        return length_ratio

    def process_image(self, image_path, ground_truth_path=None):
        """å¤„ç†å•ä¸ªå›¾åƒï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print(f"\nğŸ–¼ï¸ å¢å¼ºç‰ˆå¤„ç†: {os.path.basename(image_path)}")

        try:
            # åŠ è½½å›¾åƒ
            original_img = self.load_image_from_file(image_path)
            if original_img is None:
                return None

            # è°ƒæ•´å°ºå¯¸
            img_pil = Image.fromarray(original_img)
            processed_img = np.array(img_pil.resize((400, 400), Image.LANCZOS))
            print(f"   ğŸ“ å¤„ç†åå°ºå¯¸: {processed_img.shape}")

            # åŠ è½½Ground Truth
            gt_coastline = None
            if ground_truth_path and os.path.exists(ground_truth_path):
                gt_img = self.load_image_from_file(ground_truth_path)
                if gt_img is not None:
                    gt_resized = np.array(Image.fromarray(gt_img).resize((400, 400), Image.LANCZOS))
                    if len(gt_resized.shape) == 3:
                        gt_gray = BasicImageProcessor.rgb_to_gray(gt_resized)
                    else:
                        gt_gray = gt_resized
                    gt_coastline = (gt_gray > 127).astype(float)

            # æ­¥éª¤1: NDWI+Otsu
            print("\nğŸ“ æ­¥éª¤1: NDWI+Otsuåˆå§‹æ©è†œç”Ÿæˆ")
            ndwi_result = self.ndwi_processor.generate_initial_mask(processed_img)

            # æ­¥éª¤2: HSVæµ·åŸŸæ©è†œ
            print("\nğŸ“ æ­¥éª¤2: HSVæµ·åŸŸæ©è†œç”Ÿæˆ")
            hsv_result = self.hsv_processor.rgb_to_hsv_mask(processed_img)

            # æ­¥éª¤3: Cannyè¾¹ç¼˜æ£€æµ‹
            print("\nğŸ“ æ­¥éª¤3: Cannyè¾¹ç¼˜æ£€æµ‹")
            canny_edges = self.canny_processor.adaptive_canny(processed_img)

            # æ­¥éª¤4: èåˆåˆå§‹æµ·å²¸çº¿
            print("\nğŸ“ æ­¥éª¤4: å¤šæºä¿¡æ¯èåˆ")
            initial_coastline = self._fuse_initial_coastlines(
                ndwi_result, hsv_result, canny_edges, gt_coastline
            )

            # æ­¥éª¤5: DQN+MCTSä¼˜åŒ–
            print("\nğŸ“ æ­¥éª¤5: DQN+MCTSæ™ºèƒ½ä¼˜åŒ–")
            dqn_env = EnhancedDQNCoastlineEnvironment(processed_img, hsv_result['ocean_mask'], initial_coastline)
            dqn_agent = EnhancedDQNAgent(dqn_env)

            optimized_coastline = dqn_agent.optimize_coastline(
                max_episodes=100,
                max_steps_per_episode=180
            )

            # æ­¥éª¤6: è¿æ¥ä¿®å¤
            print("\nğŸ“ æ­¥éª¤6: æµ·å²¸çº¿è¿æ¥ä¿®å¤")
            connected_coastline = self.connection_repair.repair_coastline_connections(
                optimized_coastline, max_gap=12
            )

            # æ­¥éª¤7: CNN-LSTMæ¨¡å¼è®°å¿†ä¿®å¤
            print("\nğŸ“ æ­¥éª¤7: CNN-LSTMæ¨¡å¼è®°å¿†ä¿®å¤")
            final_coastline = self._apply_pattern_memory_repair(
                processed_img, connected_coastline, hsv_result['ocean_mask']
            )

            # æ­¥éª¤8: æœ€ç»ˆæµ·åŸŸæƒ©ç½šæ£€æŸ¥
            print("\nğŸ“ æ­¥éª¤8: æœ€ç»ˆæµ·åŸŸè½®å»“æ£€æŸ¥")
            final_coastline = self._apply_ocean_penalty(final_coastline, hsv_result['ocean_mask'])

            # æ­¥éª¤9: è´¨é‡è¯„ä¼°
            quality_metrics = self._evaluate_quality(final_coastline, gt_coastline, hsv_result['ocean_mask'])

            return {
                'original_image': original_img,
                'processed_image': processed_img,
                'ndwi_result': ndwi_result,
                'hsv_result': hsv_result,
                'canny_edges': canny_edges,
                'initial_coastline': initial_coastline,
                'optimized_coastline': optimized_coastline,
                'connected_coastline': connected_coastline,
                'final_coastline': final_coastline,
                'ground_truth': gt_coastline,
                'quality_metrics': quality_metrics,
                'success': quality_metrics['overall_score'] > 0.6
            }

        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    def batch_process(self, initial_dir, ground_truth_dir=None, max_samples=3):
        """æ‰¹é‡å¤„ç†å›¾åƒï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆæ‰¹é‡å¤„ç†...")

        if not os.path.exists(initial_dir):
            print(f"âŒ å›¾åƒç›®å½•ä¸å­˜åœ¨: {initial_dir}")
            return []

        # æ”¯æŒå¤šç§å›¾åƒæ ¼å¼
        supported_formats = ['.pdf', '.png', '.jpg', '.jpeg', '.bmp', '.tiff']
        initial_files = [f for f in os.listdir(initial_dir)
                         if any(f.lower().endswith(ext) for ext in supported_formats)]

        print(f"   æ‰¾åˆ° {len(initial_files)} ä¸ªå›¾åƒæ–‡ä»¶")

        if len(initial_files) == 0:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„å›¾åƒæ–‡ä»¶")
            return []

        # Ground Truthæ–‡ä»¶
        gt_files = []
        if ground_truth_dir and os.path.exists(ground_truth_dir):
            gt_files = [f for f in os.listdir(ground_truth_dir)
                        if any(f.lower().endswith(ext) for ext in supported_formats)]
            print(f"   æ‰¾åˆ° {len(gt_files)} ä¸ªGround Truthæ–‡ä»¶")

        results = []

        for i, img_file in enumerate(initial_files[:max_samples]):
            print(f"\n{'=' * 90}")
            print(f"å¢å¼ºç‰ˆå¤„ç†æ ·æœ¬ {i + 1}/{min(max_samples, len(initial_files))}: {img_file}")

            # åŒ¹é…Ground Truthæ–‡ä»¶
            gt_file = None
            if gt_files:
                base_name = os.path.splitext(img_file)[0]
                for gt in gt_files:
                    if base_name in gt or gt.split('.')[0] in img_file:
                        gt_file = gt
                        break

                if gt_file is None and i < len(gt_files):
                    gt_file = gt_files[i]

            # å¤„ç†å›¾åƒ
            initial_path = os.path.join(initial_dir, img_file)
            gt_path = os.path.join(ground_truth_dir, gt_file) if gt_file and ground_truth_dir else None

            if gt_path and os.path.exists(gt_path):
                print(f"   ğŸ¯ ä½¿ç”¨Ground Truth: {gt_file}")
            else:
                print(f"   âš ï¸ æœªæ‰¾åˆ°Ground Truth")

            # æ‰§è¡Œæ£€æµ‹
            result = self.process_image(initial_path, gt_path)

            if result is not None:
                result['filename'] = img_file
                result['sample_id'] = f"enhanced_sample_{i + 1}"
                results.append(result)

                # æ˜¾ç¤ºç»“æœæ‘˜è¦
                metrics = result['quality_metrics']
                print(f"âœ… {img_file} å¢å¼ºç‰ˆå¤„ç†å®Œæˆ!")
                print(f"   ç»¼åˆå¾—åˆ†: {metrics['overall_score']:.3f}")
                print(f"   æµ·å²¸çº¿åƒç´ : {metrics['coastline_pixels']:,}")
                print(f"   è¿é€šç»„ä»¶æ•°: {metrics['num_components']}")
                print(f"   è¿ç»­æ€§å¾—åˆ†: {metrics['continuity_score']:.3f}")
                print(f"   é•¿åº¦è´¨é‡: {metrics['length_quality']:.3f}")
                print(f"   æµ·åŸŸæƒ©ç½šæ¯”ä¾‹: {metrics['ocean_penalty_ratio']:.3f}")

                if 'f1_score' in metrics:
                    print(f"   F1å¾—åˆ†: {metrics['f1_score']:.3f}")
                    print(f"   IoU: {metrics['iou']:.3f}")
            else:
                print(f"âŒ {img_file} å¤„ç†å¤±è´¥")

        return results


# ==================== å¯è§†åŒ–å‡½æ•° ====================

def create_enhanced_visualization(result, save_path):
    """åˆ›å»ºå¢å¼ºç‰ˆå¯è§†åŒ–"""
    fig, axes = plt.subplots(4, 4, figsize=(22, 22))
    fig.suptitle(f'Enhanced DQN+MCTS+CNN-LSTM Coastline Detection - {result["sample_id"]}',
                 fontsize=16, fontweight='bold')

    # ç¬¬ä¸€è¡Œï¼šè¾“å…¥å’ŒåŸºç¡€å¤„ç†
    axes[0, 0].imshow(result['original_image'])
    axes[0, 0].set_title('Original Image')
    axes[0, 0].axis('off')

    axes[0, 1].imshow(result['processed_image'])
    axes[0, 1].set_title('Processed Image (400x400)')
    axes[0, 1].axis('off')

    axes[0, 2].imshow(result['ndwi_result']['ndwi'], cmap='RdYlBu')
    axes[0, 2].set_title('NDWI Map')
    axes[0, 2].axis('off')

    axes[0, 3].imshow(result['ndwi_result']['water_mask'], cmap='Blues')
    water_pixels = np.sum(result['ndwi_result']['water_mask'])
    axes[0, 3].set_title(f'NDWI+Otsu Water Mask\n({water_pixels:,} pixels)')
    axes[0, 3].axis('off')

    # ç¬¬äºŒè¡Œï¼šHSVå’Œè¾¹ç¼˜æ£€æµ‹
    axes[1, 0].imshow(result['hsv_result']['hsv_image'])
    axes[1, 0].set_title('HSV Image')
    axes[1, 0].axis('off')

    axes[1, 1].imshow(result['hsv_result']['ocean_mask'], cmap='Blues')
    ocean_pixels = np.sum(result['hsv_result']['ocean_mask'])
    axes[1, 1].set_title(f'HSV Ocean Mask\n({ocean_pixels:,} pixels)')
    axes[1, 1].axis('off')

    axes[1, 2].imshow(result['canny_edges'], cmap='gray')
    canny_pixels = np.sum(result['canny_edges'] > 0.5)
    axes[1, 2].set_title(f'Canny Edges\n({canny_pixels:,} pixels)')
    axes[1, 2].axis('off')

    axes[1, 3].imshow(result['initial_coastline'], cmap='hot')
    initial_pixels = np.sum(result['initial_coastline'] > 0.5)
    axes[1, 3].set_title(f'Fused Initial Coastline\n({initial_pixels:,} pixels)')
    axes[1, 3].axis('off')

    # ç¬¬ä¸‰è¡Œï¼šå¢å¼ºä¼˜åŒ–è¿‡ç¨‹
    axes[2, 0].imshow(result['optimized_coastline'], cmap='hot')
    opt_pixels = np.sum(result['optimized_coastline'] > 0.5)
    axes[2, 0].set_title(f'DQN+MCTS Optimized\n({opt_pixels:,} pixels)',
                         color='purple', fontweight='bold')
    axes[2, 0].axis('off')

    axes[2, 1].imshow(result['connected_coastline'], cmap='hot')
    conn_pixels = np.sum(result['connected_coastline'] > 0.5)
    axes[2, 1].set_title(f'Connection Repaired\n({conn_pixels:,} pixels)',
                         color='orange', fontweight='bold')
    axes[2, 1].axis('off')

    axes[2, 2].imshow(result['final_coastline'], cmap='hot')
    final_pixels = np.sum(result['final_coastline'] > 0.5)
    axes[2, 2].set_title(f'CNN-LSTM Enhanced\n({final_pixels:,} pixels)',
                         color='red', fontweight='bold')
    axes[2, 2].axis('off')

    # Ground Truthæ¯”è¾ƒ
    if result['ground_truth'] is not None:
        comparison = np.zeros((*result['final_coastline'].shape, 3))
        comparison[:, :, 0] = result['final_coastline']
        comparison[:, :, 1] = result['ground_truth']
        overlap = result['final_coastline'] * result['ground_truth']
        comparison[:, :, 2] = overlap

        axes[2, 3].imshow(comparison)
        axes[2, 3].set_title('Prediction vs Ground Truth\n(Red: Pred, Green: GT, Blue: Overlap)')
        axes[2, 3].axis('off')
    else:
        axes[2, 3].axis('off')
        axes[2, 3].set_title('Ground Truth\n(Not Available)')

    # ç¬¬å››è¡Œï¼šè¯¦ç»†åˆ†æ
    # å¤„ç†è¿‡ç¨‹å¯¹æ¯”
    process_comparison = np.zeros((*result['final_coastline'].shape, 3))
    process_comparison[:, :, 0] = result['initial_coastline']
    process_comparison[:, :, 1] = result['optimized_coastline']
    process_comparison[:, :, 2] = result['final_coastline']

    axes[3, 0].imshow(process_comparison)
    axes[3, 0].set_title('Process Evolution\n(Red: Initial, Green: DQN+MCTS, Blue: Final)')
    axes[3, 0].axis('off')

    # æµ·åŸŸå®‰å…¨æ£€æŸ¥
    ocean_safety = result['final_coastline'] * (1 - result['hsv_result']['ocean_mask'])
    axes[3, 1].imshow(ocean_safety, cmap='RdYlGn')
    axes[3, 1].set_title('Ocean Safety Analysis\n(Green: Safe, Red: Penalty)')
    axes[3, 1].axis('off')

    # è¿é€šæ€§åˆ†æ
    from scipy.ndimage import label
    labeled_array, num_components = label(result['final_coastline'] > 0.5)
    axes[3, 2].imshow(labeled_array, cmap='tab20')
    axes[3, 2].set_title(f'Connectivity Analysis\n({num_components} components)')
    axes[3, 2].axis('off')

    # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    axes[3, 3].axis('off')

    metrics = result['quality_metrics']
    stats_text = f"""Enhanced DQN+MCTS+CNN-LSTM Results:

Overall Score: {metrics['overall_score']:.3f}
Status: {"âœ… SUCCESS" if result['success'] else "âŒ FAILED"}

Coastline Analysis:
â€¢ Final pixels: {metrics['coastline_pixels']:,}
â€¢ Coverage: {metrics['coverage_ratio'] * 100:.1f}%
â€¢ Components: {metrics['num_components']}
â€¢ Continuity score: {metrics['continuity_score']:.3f}
â€¢ Length quality: {metrics['length_quality']:.3f}

Ocean Penalty System:
â€¢ Ocean penalty pixels: {metrics['ocean_penalty_pixels']:,}
â€¢ Ocean penalty ratio: {metrics['ocean_penalty_ratio']:.3f}
â€¢ Status: {"âš ï¸ HIGH" if metrics['ocean_penalty_ratio'] > 0.1 else "âœ… LOW"}

Enhanced Processing Pipeline:
âœ“ NDWI+Otsu water detection
âœ“ HSV ocean mask generation
âœ“ Adaptive Canny edge detection
âœ“ Multi-source information fusion
âœ“ DQN+MCTS hybrid optimization
  - Deep Q-Network policy
  - Monte Carlo Tree Search
  - Edge-focused MCTS deployment
âœ“ Connection repair algorithm
âœ“ CNN-LSTM pattern memory
âœ“ Ocean penalty enforcement

Hybrid AI Architecture:
â€¢ DQN: 5-channel CNN + FC layers
â€¢ MCTS: UCB1 selection, 50 iterations
â€¢ CNN-LSTM: Pattern memory network
â€¢ Connection repair: Gap bridging
â€¢ Device: {device}"""

    if 'f1_score' in metrics:
        stats_text += f"""

Accuracy Metrics:
â€¢ Precision: {metrics['precision']:.3f}
â€¢ Recall: {metrics['recall']:.3f}
â€¢ F1-Score: {metrics['f1_score']:.3f}
â€¢ IoU: {metrics['iou']:.3f}"""

    # æ·»åŠ æ”¹è¿›ç»Ÿè®¡
    initial_pixels = np.sum(result['initial_coastline'] > 0.5)
    optimized_pixels = np.sum(result['optimized_coastline'] > 0.5)
    connected_pixels = np.sum(result['connected_coastline'] > 0.5)
    final_pixels = metrics['coastline_pixels']

    dqn_improvement = ((optimized_pixels - initial_pixels) / max(initial_pixels, 1)) * 100
    connection_improvement = ((connected_pixels - optimized_pixels) / max(optimized_pixels, 1)) * 100
    total_improvement = ((final_pixels - initial_pixels) / max(initial_pixels, 1)) * 100

    stats_text += f"""

Enhancement Statistics:
â€¢ Initial pixels: {initial_pixels:,}
â€¢ DQN+MCTS pixels: {optimized_pixels:,}
â€¢ Connected pixels: {connected_pixels:,}
â€¢ Final pixels: {final_pixels:,}
â€¢ DQN+MCTS improvement: {dqn_improvement:+.1f}%
â€¢ Connection improvement: {connection_improvement:+.1f}%
â€¢ Total improvement: {total_improvement:+.1f}%"""

    axes[3, 3].text(0.02, 0.98, stats_text, transform=axes[3, 3].transAxes,
                    fontsize=6, verticalalignment='top', fontfamily='monospace',
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightcyan", alpha=0.9))
    axes[3, 3].set_title('Enhanced Detection Statistics', fontsize=10)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()

    print(f"âœ… å¢å¼ºç‰ˆå¯è§†åŒ–å·²ä¿å­˜: {save_path}")


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆDQN+MCTS+CNN-LSTMæµ·å²¸çº¿æ£€æµ‹ç³»ç»Ÿ...")

    # æ£€æŸ¥PyTorch
    print(f"\nğŸ”§ PyTorchç¯å¢ƒæ£€æŸ¥:")
    print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"   è®¾å¤‡: {device}")
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")

    # æ£€æŸ¥å…¶ä»–ä¾èµ–
    print("\nğŸ” æ£€æŸ¥ä¾èµ–åº“...")
    print(f"   PyMuPDF (PDFæ”¯æŒ): {'âœ…' if HAS_PDF_SUPPORT else 'âŒ'}")
    print(f"   scikit-image: {'âœ…' if HAS_SKIMAGE else 'âŒ (ä½¿ç”¨åŸºç¡€å®ç°)'}")
    print(f"   scikit-learn: {'âœ…' if HAS_SKLEARN else 'âŒ (è·³è¿‡DBSCAN)'}")

    detector = EnhancedCoastlineDetector()

    # è®¾ç½®è·¯å¾„
    initial_dir = "E:/initial"  # åŸå§‹å›¾åƒç›®å½•
    ground_truth_dir = "E:/ground"  # Ground Truthç›®å½•ï¼ˆå¯é€‰ï¼‰

    print(f"\nğŸ“ æ£€æŸ¥æ•°æ®ç›®å½•...")
    print(f"   åŸå§‹å›¾åƒ: {initial_dir}")
    print(f"   Ground Truth: {ground_truth_dir}")

    # æ‰¹é‡å¤„ç†
    results = detector.batch_process(initial_dir, ground_truth_dir, max_samples=3)

    if not results:
        print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ ·æœ¬")
        return

    # ä¿å­˜ç»“æœ
    output_dir = "./enhanced_dqn_mcts_lstm_results"
    os.makedirs(output_dir, exist_ok=True)

    print(f"\nğŸ’¾ ä¿å­˜å¢å¼ºç‰ˆå¯è§†åŒ–ç»“æœ...")
    for result in results:
        save_path = os.path.join(output_dir, f'enhanced_coastline_{result["sample_id"]}.png')
        create_enhanced_visualization(result, save_path)

    # æ€»ç»“æŠ¥å‘Š
    print(f"\nğŸ‰ å¢å¼ºç‰ˆæµ·å²¸çº¿æ£€æµ‹å®Œæˆ!")
    print(f"ğŸ“‚ ç»“æœä¿å­˜åœ¨: {output_dir}")

    successful = [r for r in results if r['success']]
    success_rate = len(successful) / len(results) * 100

    print(f"\nğŸ“Š å¢å¼ºç‰ˆå¤„ç†æ€»ç»“:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(results)}")
    print(f"   æˆåŠŸå¤„ç†: {len(successful)} ({success_rate:.1f}%)")

    if successful:
        avg_score = np.mean([r['quality_metrics']['overall_score'] for r in successful])
        avg_pixels = np.mean([r['quality_metrics']['coastline_pixels'] for r in successful])
        avg_components = np.mean([r['quality_metrics']['num_components'] for r in successful])
        avg_continuity = np.mean([r['quality_metrics']['continuity_score'] for r in successful])
        avg_length_quality = np.mean([r['quality_metrics']['length_quality'] for r in successful])

        print(f"   å¹³å‡ç»¼åˆå¾—åˆ†: {avg_score:.3f}")
        print(f"   å¹³å‡æµ·å²¸çº¿åƒç´ : {avg_pixels:,.0f}")
        print(f"   å¹³å‡è¿é€šç»„ä»¶æ•°: {avg_components:.1f}")
        print(f"   å¹³å‡è¿ç»­æ€§å¾—åˆ†: {avg_continuity:.3f}")
        print(f"   å¹³å‡é•¿åº¦è´¨é‡: {avg_length_quality:.3f}")

        with_accuracy = [r for r in successful if 'f1_score' in r['quality_metrics']]
        if with_accuracy:
            avg_f1 = np.mean([r['quality_metrics']['f1_score'] for r in with_accuracy])
            avg_iou = np.mean([r['quality_metrics']['iou'] for r in with_accuracy])
            print(f"   å¹³å‡F1å¾—åˆ†: {avg_f1:.3f}")
            print(f"   å¹³å‡IoU: {avg_iou:.3f}")

    print(f"\nğŸ’¡ å¢å¼ºç‰ˆç³»ç»Ÿç‰¹æ€§:")
    print(f"   ğŸ§  Deep Q-Network (DQN)")
    print(f"   ğŸŒ³ Monte Carlo Tree Search (MCTS)")
    print(f"   ğŸ”— CNN-LSTM Pattern Memory")
    print(f"   ğŸ”§ Connection Repair Algorithm")
    print(f"   ğŸ¯ Edge-focused MCTS deployment")
    print(f"   ğŸ“Š Enhanced reward function")
    print(f"   ğŸŒŠ Advanced ocean penalty system")
    print(f"   ğŸ“ˆ Multi-stage optimization pipeline")
    print(f"   ğŸ’» CPU/GPUè‡ªé€‚åº”è¿è¡Œ")


if __name__ == "__main__":
    print("ğŸ” å¢å¼ºç‰ˆä¾èµ–æ£€æŸ¥...")

    # æ£€æŸ¥PyTorch
    try:
        import torch
        import torch.nn as nn

        print(f"âœ… PyTorch {torch.__version__} æ£€æŸ¥é€šè¿‡")
    except ImportError:
        print("âŒ ç¼ºå°‘PyTorch")
        print("è¯·å®‰è£…: pip install torch torchvision")
        exit(1)

    # æ£€æŸ¥åŸºç¡€ä¾èµ–
    required_packages = ['numpy', 'matplotlib', 'PIL', 'scipy']
    optional_packages = ['fitz', 'skimage']

    missing_required = []
    for pkg in required_packages:
        try:
            __import__(pkg if pkg != 'PIL' else 'PIL.Image')
        except ImportError:
            missing_required.append(pkg)

    if missing_required:
        print(f"âŒ ç¼ºå°‘å¿…éœ€ä¾èµ–: {', '.join(missing_required)}")
        print("è¯·å®‰è£…: pip install numpy matplotlib pillow scipy")
    else:
        print("âœ… åŸºç¡€ä¾èµ–æ£€æŸ¥é€šè¿‡")

        missing_optional = []
        for pkg in optional_packages:
            try:
                if pkg == 'fitz':
                    import fitz
                elif pkg == 'skimage':
                    from skimage import filters
            except ImportError:
                missing_optional.append(pkg)

        if missing_optional:
            print(f"âš ï¸ å¯é€‰ä¾èµ–ç¼ºå¤±: {', '.join(missing_optional)}")
            print("å»ºè®®å®‰è£…: pip install PyMuPDF scikit-image")

        print("\n" + "=" * 70)
        main()